# BI-Solution-for-Lemonade

Code used to analyze, clean, process, visualize, and model insurance claims data in order to predict fraudulent claims

Dataset retreived from Kaggle which includes 1000 instances of auto insurance claims data

The code begins by loading in the dataset and generating summary statistics, a confusion matrix, and count plots of the variables divided by fraud_reported. These things can provide some insights into the dataset and can be used to clean the dataset. Categorical variables with a high number of unique values are not useful in predictions, so they must be dropped. Likewise, any variables that may cause multicollinearity must also be dropped. After the data has been cleaned, it must be processed so it can be used for the model. Some variable categories are modified and categorical variables are encoded. Density histograms and box plots are then generated in order to check the data quality and the presence of outliers. Since the data quality looks good but outliers are apparent, numerical variables are scaled and standardized. 
The data is split into testing and training datasets and the models are created. After the models have been fit, accuracy metrics, a confusion matrix, and AUC scores are generated in order to evaluate the models. The decision tree model is created using the sklearn DecisionTreeClassifier. It was created using the CART algorithm and thus the Gini index was used to create split points. The initial tree overfitted the data, so it was pruned using cost complexity pruning. Next, the SVC model is created using the sklearn.svm module. Hyperparameter tuning is attempted in order to improve it, but to no avail. The next model generated is the K-Nearest Neighbors model, created with the sklearn KNeighborsClassifier function. Error rates vs the number of neighbors are plotted with seaborn in order to find the ideal number of neighbors. Finally, the Gaussian Naive Bayes model is created with the sklearn GaussianNB function. Forward sequential feature selection is performed using the mlxtend SequentialFeatureSelector function in order to select 12 features that the model will use. After the models are created and fit statistics are generated, a ROC curve for all models is generated using matplotlib. 
